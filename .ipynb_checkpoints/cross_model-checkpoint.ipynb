{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "seed = 0\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues, font_size=10, fig_size=(12,10)):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    #classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    #else:\n",
    "    #    print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "    plt.rcParams.update({'font.size': font_size})\n",
    "    fig, ax = plt.subplots(figsize=fig_size, dpi= 80, facecolor='w', edgecolor='k')\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Label Indexes\n",
    "This chunk loads a file that contains the labels we want to load from the datasets as well as their indicies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"tstat_labels_indexes.txt\" ,'r')\n",
    "data_field_list = []\n",
    "for line in infile.readlines():\n",
    "    if \":\" in line:\n",
    "        data_field = str(re.search('%s(.*)%s' % (\"\\\"\", \"\\\"\"), line).group(1))\n",
    "        index = int(re.search('%s(.*)%s' % (\":\", \",\"), line).group(1))\n",
    "        data_field_list.append((data_field, index))\n",
    "\n",
    "index_to_key_dict = {}\n",
    "key_to_index_dict = {}\n",
    "data_field_labels = []\n",
    "for data_field, index in data_field_list:\n",
    "    key_to_index_dict[data_field] = index\n",
    "    index_to_key_dict[index] = data_field\n",
    "    data_field_labels.append(data_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in a dataset file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_file(file_name):\n",
    "    infile = open(file_name, 'r')\n",
    "    header = infile.readline().split(' ')\n",
    "    entries = []\n",
    "    labels = None\n",
    "    for i, line in enumerate(infile.readlines()):\n",
    "        row = get_data_row(line)\n",
    "        row = clean_data_row(row)\n",
    "        if row != []:\n",
    "            entries.append(row)\n",
    "    entries = np.array(entries)\n",
    "    return entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data row\n",
    "Called by the read in file function. Loads a single line from the dataset files. Super inefficient, but only loads labels which are in the data field list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_row(line):\n",
    "    global index_to_key_dict\n",
    "    line = line.split(' ')\n",
    "    row = []\n",
    "    labels = []\n",
    "    c_pkt_cnt = 0\n",
    "    s_pkt_cnt = 0\n",
    "    c_bytes_cnt = 0\n",
    "    s_bytes_cnt = 0\n",
    "    for data_field, index in data_field_list:\n",
    "        #print(\"df:\", data_field,\"ix:\",index)\n",
    "        #print(line)\n",
    "        \n",
    "        \n",
    "        if data_field == \"client_pkt_cnt\":\n",
    "            try:\n",
    "                c_pkt_cnt = line[index]\n",
    "                c_pkt_cnt = max(float(c_pkt_cnt), 1)\n",
    "            except:\n",
    "                c_pkt_cnt = 1\n",
    "            #if c_pkt_cnt < 32:\n",
    "            #    return []\n",
    "        elif data_field == \"serv_pkt_cnt\":\n",
    "            try:\n",
    "                s_pkt_cnt = line[index]\n",
    "                s_pkt_cnt = max(float(s_pkt_cnt), 1)\n",
    "            except:\n",
    "                s_pkt_cnt = 1\n",
    "        elif data_field == \"client_bytes_cnt\":\n",
    "            try:\n",
    "                c_bytes_cnt = line[index]\n",
    "                c_bytes_cnt = max(float(c_bytes_cnt), 1)\n",
    "            except:\n",
    "                c_bytes_cnt = 1\n",
    "        elif data_field == \"serv_bytes_cnt\":\n",
    "            try:\n",
    "                s_bytes_cnt = line[index]\n",
    "                s_bytes_cnt = max(float(s_bytes_cnt), 1)\n",
    "            except:\n",
    "                s_bytes_cnt = 1\n",
    "                \n",
    "    for data_field, index in data_field_list:\n",
    "        try:\n",
    "            val = line[index]\n",
    "            val = float(val)\n",
    "        except:\n",
    "            val = 0\n",
    "        if data_field in [\"client_pkt_cnt\", \"client_rst_cnt\", \"client_ack_cnt\", \"client_pkt_data\", \"client_pkt_retx\",\n",
    "                         \"client_syn_cnt\", \"client_fin_cnt\", \"client_pkt_retx\"]:\n",
    "            val /= c_pkt_cnt\n",
    "        elif data_field in [\"client_bytes_uniq\", \"client_bytes_cnt\", \"client_bytes_retx\"]:\n",
    "            val /= c_bytes_cnt\n",
    "        elif data_field in [\"serv_pkt_cnt\", \"serv_rst_cnt\", \"serv_ack_cnt\", \"serv_ack_pck_cnt\", \"serv_pkts_data\", \n",
    "                            \"serv_pkts_retx\", \"serv_syn_cnt\", \"serv_fin_cnt\"]:\n",
    "            val /= s_pkt_cnt\n",
    "        elif data_field in [\"serv_bytes_uniq\", \"serv_btyes_cnt\", \"serv_pkts_retx\"]:\n",
    "            val /= s_bytes_cnt\n",
    "        row.append(val)    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data row\n",
    "Not implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data_row(in_row):\n",
    "    global index_to_key_dict, key_to_index_dict\n",
    "    return in_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get dataset\n",
    "Loads all files from a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(path):\n",
    "    print(path)\n",
    "    out_data = []\n",
    "    for sub_dir in os.listdir(path):\n",
    "        temp_path = os.path.join(path, sub_dir)\n",
    "        temp_path = os.path.join(temp_path, \"log_tcp_complete\")\n",
    "        if os.path.isfile(temp_path): \n",
    "            temp_data = read_in_file(temp_path)\n",
    "            #print len(temp_data), len(out_data)\n",
    "            if len(temp_data) == 0:\n",
    "                continue\n",
    "            if out_data == []:\n",
    "                out_data = temp_data\n",
    "            else:\n",
    "                out_data = np.concatenate((out_data, temp_data))\n",
    "    return out_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load HPC datasets\n",
    "Load all datasets\n",
    "Create numerical lables for each class, and a different set of labels for each subclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./hpc/normal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dave/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./hpc/corrupt_0.1perc\n",
      "./hpc/corrupt_0.5perc\n",
      "./hpc/corrupt_1.0perc\n",
      "./hpc/delay_1_var_1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "normal = get_dataset(\"./hpc/normal\")\n",
    "corr_01 = get_dataset(\"./hpc/corrupt_0.1perc\")\n",
    "corr_05 = get_dataset(\"./hpc/corrupt_0.5perc\")\n",
    "corr_10 = get_dataset(\"./hpc/corrupt_1.0perc\")\n",
    "delay_1_1 = get_dataset(\"./hpc/delay_1_var_1\")\n",
    "delay_5_2 = get_dataset(\"./hpc/delay_5_var_2\")\n",
    "delay_10_5 = get_dataset(\"./hpc/delay_10_var_5\")\n",
    "delay_25_20 = get_dataset(\"./hpc/delay_25_var_20\")\n",
    "drop_01 = get_dataset(\"./hpc/drop_01_perc\")\n",
    "drop_001 = get_dataset(\"./hpc/drop_001_perc\")\n",
    "drop_0005 = get_dataset(\"./hpc/drop_0005_perc\")\n",
    "dup_1 = get_dataset(\"./hpc/dup-1-p\")\n",
    "dup_2 = get_dataset(\"./hpc/dup_2perc\")\n",
    "\n",
    "hpc_normal = np.nan_to_num(normal)\n",
    "hpc_corr_01 = np.nan_to_num(corr_01)\n",
    "hpc_corr_05 =  np.nan_to_num(corr_05)\n",
    "hpc_corr_10 = np.nan_to_num(corr_10)\n",
    "hpc_delay_1_1 = np.nan_to_num(delay_1_1)\n",
    "hpc_delay_5_2 = np.nan_to_num(delay_5_2)\n",
    "hpc_delay_10_5 = np.nan_to_num(delay_10_5)\n",
    "hpc_delay_25_20 = np.nan_to_num(delay_25_20)\n",
    "hpc_drop_01 = np.nan_to_num(drop_01)\n",
    "hpc_drop_001 = np.nan_to_num(drop_001)\n",
    "hpc_drop_0005 = np.nan_to_num(drop_0005)\n",
    "hpc_dup_1 = np.nan_to_num(dup_1)\n",
    "hpc_dup_2 = np.nan_to_num(dup_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = get_dataset(\"./dtn/FINAL_DATA/normal\")\n",
    "normal_2 = get_dataset(\"./dtn/DTN_LONG_DATA/normal\")\n",
    "corr_01 = get_dataset(\"./dtn/FINAL_DATA/corrupt_0.1perc\")\n",
    "corr_05 = get_dataset(\"./dtn/FINAL_DATA/corrupt_0.5perc\")\n",
    "corr_10 = get_dataset(\"./dtn/FINAL_DATA/corrupt_1.0perc\")\n",
    "delay_1_1 = get_dataset(\"./dtn/FINAL_DATA/delay_1_var_1\")\n",
    "delay_5_2 = get_dataset(\"./dtn/FINAL_DATA/delay_5_var_2\")\n",
    "delay_10_5 = get_dataset(\"./dtn/FINAL_DATA/delay_10_var_5\")\n",
    "delay_25_20 = get_dataset(\"./dtn/FINAL_DATA/delay_25_var_20\")\n",
    "drop_01 = get_dataset(\"./dtn/FINAL_DATA/drop_01_perc\")\n",
    "drop_01_2 = get_dataset(\"./dtn/DTN_LONG_DATA/one_perc\")\n",
    "drop_001 = get_dataset(\"./dtn/FINAL_DATA/drop_001_perc\")\n",
    "drop_001_2 = get_dataset(\"./dtn/DTN_LONG_DATA/tenth_perc\")\n",
    "drop_0005 = get_dataset(\"./dtn/FINAL_DATA/drop_0005_perc\")\n",
    "drop_0005_2 = get_dataset(\"./dtn/DTN_LONG_DATA/half_perc\")\n",
    "dup_01 = get_dataset(\"./dtn/FINAL_DATA/dup_0.1perc\")\n",
    "dup_1 = get_dataset(\"./dtn/FINAL_DATA/dup_1perc\")\n",
    "dup_2 = get_dataset(\"./dtn/FINAL_DATA/dup_2perc\")\n",
    "\n",
    "dtn_normal = np.nan_to_num(normal)\n",
    "dtn_normal2 = np.nan_to_num(normal_2)\n",
    "dtn_corr_01 = np.nan_to_num(corr_01)\n",
    "dtn_corr_05 =  np.nan_to_num(corr_05)\n",
    "dtn_corr_10 = np.nan_to_num(corr_10)\n",
    "dtn_delay_1_1 = np.nan_to_num(delay_1_1)\n",
    "dtn_delay_5_2 = np.nan_to_num(delay_5_2)\n",
    "dtn_delay_10_5 = np.nan_to_num(delay_10_5)\n",
    "dtn_delay_25_20 = np.nan_to_num(delay_25_20)\n",
    "dtn_drop_01 = np.nan_to_num(drop_01)\n",
    "dtn_drop_01_2 = np.nan_to_num(drop_01_2)\n",
    "dtn_drop_001 = np.nan_to_num(drop_001)\n",
    "dtn_drop_001_2 = np.nan_to_num(drop_001_2)\n",
    "dtn_drop_0005 = np.nan_to_num(drop_0005)\n",
    "dtn_drop_0005_2 = np.nan_to_num(drop_0005_2)\n",
    "dtn_dup_01 = np.nan_to_num(dup_01)\n",
    "dtn_dup_1 = np.nan_to_num(dup_1)\n",
    "dtn_dup_2 = np.nan_to_num(dup_2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hpc_data = np.concatenate((hpc_normal, \n",
    "                           hpc_corr_01, hpc_corr_05, hpc_corr_10,\n",
    "                           hpc_delay_1_1, hpc_delay_5_2,hpc_delay_10_5,hpc_delay_25_20,\n",
    "                           #hpc_drop_01, hpc_drop_001, hpc_drop_0005,\n",
    "                           hpc_dup_1, hpc_dup_2))\n",
    "\n",
    "hpc_data = StandardScaler().fit_transform(hpc_data)\n",
    "\n",
    "\n",
    "\n",
    "dtn_data = np.concatenate((dtn_normal, dtn_normal2,\n",
    "                           dtn_corr_01, dtn_corr_05, dtn_corr_10,\n",
    "                           dtn_delay_1_1, dtn_delay_5_2,dtn_delay_10_5,dtn_delay_25_20,\n",
    "                           #dtn_drop_01,dtn_drop_01_2, dtn_drop_001, dtn_drop_001_2, dtn_drop_0005, dtn_drop_0005_2,                          \n",
    "                           dtn_dup_01,dtn_dup_1, dtn_dup_2))\n",
    "\n",
    "dtn_data = StandardScaler().fit_transform(dtn_data)\n",
    "\n",
    "all_data = np.concatenate((hpc_normal, \n",
    "                           hpc_corr_01, hpc_corr_05, hpc_corr_10,\n",
    "                           hpc_delay_1_1, hpc_delay_5_2,hpc_delay_10_5,hpc_delay_25_20,\n",
    "                           #hpc_drop_01, hpc_drop_001, hpc_drop_0005,\n",
    "                           hpc_dup_1, hpc_dup_2, \n",
    "                           dtn_normal, dtn_normal2,\n",
    "                           dtn_corr_01, dtn_corr_05, dtn_corr_10,\n",
    "                           dtn_delay_1_1, dtn_delay_5_2,dtn_delay_10_5,dtn_delay_25_20,\n",
    "                           #dtn_drop_01,dtn_drop_01_2, dtn_drop_001, dtn_drop_001_2, dtn_drop_0005, dtn_drop_0005_2,                          \n",
    "                           dtn_dup_01,dtn_dup_1, dtn_dup_2))\n",
    "\n",
    "all_data = StandardScaler().fit_transform(all_data)\n",
    "\n",
    "\n",
    "dtn_normal = np.nan_to_num(normal)\n",
    "dtn_normal2 = np.nan_to_num(normal_2)\n",
    "dtn_corr_01 = np.nan_to_num(corr_01)\n",
    "dtn_corr_05 =  np.nan_to_num(corr_05)\n",
    "dtn_corr_10 = np.nan_to_num(corr_10)\n",
    "dtn_delay_1_1 = np.nan_to_num(delay_1_1)\n",
    "dtn_delay_5_2 = np.nan_to_num(delay_5_2)\n",
    "dtn_delay_10_5 = np.nan_to_num(delay_10_5)\n",
    "dtn_delay_25_20 = np.nan_to_num(delay_25_20)\n",
    "dtn_drop_01 = np.nan_to_num(drop_01)\n",
    "dtn_drop_01_2 = np.nan_to_num(drop_01_2)\n",
    "dtn_drop_001 = np.nan_to_num(drop_001)\n",
    "dtn_drop_001_2 = np.nan_to_num(drop_001_2)\n",
    "dtn_drop_0005 = np.nan_to_num(drop_0005)\n",
    "dtn_drop_0005_2 = np.nan_to_num(drop_0005_2)\n",
    "dtn_dup_01 = np.nan_to_num(dup_01)\n",
    "dtn_dup_1 = np.nan_to_num(dup_1)\n",
    "dtn_dup_2 = np.nan_to_num(dup_2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_labels  = np.ones(len(hpc_normal  ))    *1\n",
    "b_labels  = np.ones(len(hpc_corr_01  ))   *2\n",
    "c_labels  = np.ones(len(hpc_corr_05  ))   *2\n",
    "d_labels  = np.ones(len(hpc_corr_10  ))   *2\n",
    "e_labels  = np.ones(len(hpc_delay_1_1))   *3\n",
    "f_labels  = np.ones(len(hpc_delay_5_2))   *3\n",
    "g_labels  = np.ones(len(hpc_delay_10_5))  *3\n",
    "h_labels  = np.ones(len(hpc_delay_25_20)) *3\n",
    "#i_labels  = np.ones(len(hpc_drop_01))     *4\n",
    "#j_labels  = np.ones(len(hpc_drop_001))    *4\n",
    "#k_labels  = np.ones(len(hpc_drop_0005))   *4\n",
    "m_labels  = np.ones(len(hpc_dup_1))       *5\n",
    "n_labels  = np.ones(len(hpc_dup_2))       *5\n",
    "\n",
    "hpc_anom_type_data_labels = np.concatenate((a_labels, b_labels, c_labels, d_labels, e_labels, \n",
    "                                            f_labels, g_labels, h_labels, \n",
    "                                            #i_labels, j_labels, k_labels, \n",
    "                                            m_labels, n_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_labels  = np.ones(len(dtn_normal  ) + len(dtn_normal2))    *1\n",
    "p_labels  = np.ones(len(dtn_corr_01  ))   *2\n",
    "q_labels  = np.ones(len(dtn_corr_05  ))   *2\n",
    "r_labels  = np.ones(len(dtn_corr_10  ))   *2\n",
    "s_labels  = np.ones(len(dtn_delay_1_1))   *3\n",
    "t_labels  = np.ones(len(dtn_delay_5_2))   *3\n",
    "u_labels  = np.ones(len(dtn_delay_10_5))  *3\n",
    "v_labels  = np.ones(len(dtn_delay_25_20)) *3\n",
    "#w_labels  = np.ones(len(dtn_drop_01 ) + len(dtn_drop_01_2))     *4\n",
    "#x_labels  = np.ones(len(dtn_drop_001) + len(dtn_drop_001_2))    *4\n",
    "#y_labels  = np.ones(len(dtn_drop_0005) + len(dtn_drop_0005_2))   *4\n",
    "z_labels  = np.ones(len(dtn_dup_01))      *5\n",
    "aa_labels  = np.ones(len(dtn_dup_1))       *5\n",
    "bb_labels  = np.ones(len(dtn_dup_2))       *5\n",
    "\n",
    "dtn_anom_type_data_labels = np.concatenate((o_labels, p_labels, q_labels, r_labels, s_labels, \n",
    "                                            t_labels, u_labels, v_labels, \n",
    "                                            #w_labels, x_labels,y_labels,\n",
    "                                            z_labels, \n",
    "                                            aa_labels, bb_labels))\n",
    "\n",
    "all_anom_type_data_labels = np.concatenate((a_labels, b_labels, c_labels, d_labels, e_labels, \n",
    "                                            f_labels, g_labels, h_labels, \n",
    "                                            #i_labels, j_labels, k_labels, \n",
    "                                            m_labels, n_labels,\n",
    "                                            o_labels, p_labels, q_labels, r_labels, s_labels, \n",
    "                                            t_labels, u_labels, v_labels, \n",
    "                                            #w_labels, x_labels,y_labels,\n",
    "                                            z_labels, \n",
    "                                            aa_labels, bb_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dtn_anom_type_data_labels.shape\n",
    "dtn_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Datasets (randomized on seed value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dtn_train_data, dtn_test_data, dtn_train_labels, dtn_test_labels = train_test_split(dtn_data, \n",
    "                                                                    dtn_anom_type_data_labels, test_size=0.45, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpc_train_data, hpc_test_data, hpc_train_labels, hpc_test_labels = train_test_split(hpc_data, \n",
    "                                                                    hpc_anom_type_data_labels, test_size=0.45, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_data, all_test_data, all_train_labels, all_test_labels = train_test_split(all_data, \n",
    "                                                                    all_anom_type_data_labels, test_size=0.45, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Train SVMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtn = svm.SVC(kernel='linear', gamma ='auto', max_iter=1000000000, class_weight='balanced')\n",
    "dtn.fit(dtn_train_data, dtn_train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpc = svm.SVC(kernel='linear', gamma ='auto', max_iter=1000000000, class_weight='balanced')\n",
    "hpc.fit(hpc_train_data, hpc_train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ = svm.SVC(kernel='linear', gamma ='auto', max_iter=1000000000, class_weight='balanced')\n",
    "all_.fit(all_train_data, all_train_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DTN Standalone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HPC Standalone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HPC Data on DTN trained SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predicted_labels = clf.predict(hpc_test_data)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix \n",
    "\n",
    "class_names = [\"Normal\", \"Corrupt\", \"Delay\", \"Duplicate\"]\n",
    "\n",
    "plot_confusion_matrix(hpc_test_labels, predicted_labels, normalize=True,classes=class_names, title='Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "error_cnt = 0\n",
    "total_error_cnt = 0\n",
    "same_class_error = 0\n",
    "for i in range(len(predicted_labels)):\n",
    "    if predicted_labels[i] != hpc_test_labels[i]:\n",
    "        total_error_cnt += 1\n",
    "        # if its normal data\n",
    "        if predicted_labels[i] == 1 or hpc_test_labels[i] == 1:\n",
    "            error_cnt += 1\n",
    "\n",
    "\n",
    "print (\"Total Error Count : \", total_error_cnt)\n",
    "print (\"Normal Class Error Rate : \", float(error_cnt)/float(len(predicted_labels)) * 100, \"%\")\n",
    "print (\"Total Error Rate : \", float(total_error_cnt)/float(len(predicted_labels)) * 100, \"%\")\n",
    "\n",
    "\n",
    "false_pos = 0\n",
    "false_neg = 0\n",
    "error_cnt = 0\n",
    "for i in range(len(predicted_labels)):\n",
    "    # if there's an error\n",
    "    if predicted_labels[i] != hpc_test_labels[i]:\n",
    "        error_cnt += 1\n",
    "        # if we failed to detect anomaly\n",
    "        if predicted_labels[i] == 1:\n",
    "            false_neg += 1\n",
    "        # detected anomaly, but it normal\n",
    "        else:\n",
    "            false_pos += 1\n",
    "print (\"Total Errors\", error_cnt)\n",
    "print (\"False Positives \", false_pos/ float(len(predicted_labels)) * 100, \"%\")\n",
    "print (\"False Negatives \", false_neg/ float(len(predicted_labels)) * 100, \"%\")\n",
    "\n",
    "\n",
    "shortened_predicted = []\n",
    "shortened_test_label = []\n",
    "for i in range(len(predicted_labels)):\n",
    "    if predicted_labels[i] == 1:\n",
    "        shortened_predicted.append(1)\n",
    "    else:\n",
    "        shortened_predicted.append(2)\n",
    "    if hpc_test_labels[i] == 1:\n",
    "        shortened_test_label.append(1)\n",
    "    else:\n",
    "        shortened_test_label.append(2)\n",
    "\n",
    "short_class_names = [\"Normal\", \"Anomalous\"]        \n",
    "        \n",
    "    \n",
    "    \n",
    "plot_confusion_matrix(shortened_test_label, shortened_predicted, normalize=True,classes=short_class_names, \n",
    "                      title='Confusion Matrix', font_size=18, fig_size=(6,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    ">>> clf.fit(X, y)  \n",
    "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
    "        learning_rate=1.0, n_estimators=100, random_state=0)\n",
    ">>> clf.feature_importances_  \n",
    "array([0.28..., 0.42..., 0.14..., 0.16...])\n",
    ">>> clf.predict([[0, 0, 0, 0]])\n",
    "array([1])all_predicted_labels = all_.predict(all_test_data)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix \n",
    "\n",
    "plot_confusion_matrix(all_test_labels, all_predicted_labels, normalize=True,classes=class_names, title='Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "error_cnt = 0\n",
    "total_error_cnt = 0\n",
    "same_class_error = 0\n",
    "\n",
    "for i in range(len(all_predicted_labels)):\n",
    "    if all_predicted_labels[i] != all_test_labels[i]:\n",
    "        total_error_cnt += 1\n",
    "        # if its normal data\n",
    "        if all_predicted_labels[i] == 1 or all_test_labels[i] == 1:\n",
    "            error_cnt += 1\n",
    "\n",
    "\n",
    "print (\"Total Error Count : \", total_error_cnt)\n",
    "print (\"Normal Class Error Rate : \", float(error_cnt)/float(len(all_predicted_labels)) * 100, \"%\")\n",
    "print (\"Total Error Rate : \", float(total_error_cnt)/float(len(all_predicted_labels)) * 100, \"%\")\n",
    "\n",
    "\n",
    "false_pos = 0\n",
    "false_neg = 0\n",
    "error_cnt = 0\n",
    "for i in range(len(all_predicted_labels)):\n",
    "    # if there's an error\n",
    "    if all_predicted_labels[i] != all_test_labels[i]:\n",
    "        error_cnt += 1\n",
    "        # if we failed to detect anomaly\n",
    "        if all_predicted_labels[i] == 1:\n",
    "            false_neg += 1\n",
    "        # detected anomaly, but it normal\n",
    "        else:\n",
    "            false_pos += 1\n",
    "print (\"Total Errors\", error_cnt)\n",
    "print (\"False Positives \", false_pos/ float(len(all_predicted_labels)) * 100, \"%\")\n",
    "print (\"False Negatives \", false_neg/ float(len(all_predicted_labels)) * 100, \"%\")\n",
    "\n",
    "all_shortened_predicted = []\n",
    "all_shortened_test_label = []\n",
    "\n",
    "for i in range(len(all_predicted_labels)):\n",
    "    if all_predicted_labels[i] == 1:\n",
    "        all_shortened_predicted.append(1)\n",
    "    else:\n",
    "        all_shortened_predicted.append(2)\n",
    "    if all_test_labels[i] == 1:\n",
    "        all_shortened_test_label.append(1)\n",
    "    else:\n",
    "        all_shortened_test_label.append(2)\n",
    "\n",
    "plot_confusion_matrix(all_shortened_test_label, all_shortened_predicted, normalize=True,classes=short_class_names, \n",
    "                      title='Confusion Matrix', font_size=18, fig_size=(6,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "clf.fit(dtn_train_data, dtn_train_labels)\n",
    "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
    "        learning_rate=1.0, n_estimators=100, random_state=0)\n",
    "clf.feature_importances_  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "clf_predicted_labels = clf.predict(dtn_test_data)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix \n",
    "\n",
    "class_names = [\"Normal\", \"Corrupt\", \"Delay\", \"Duplicate\"]\n",
    "\n",
    "plot_confusion_matrix(hpc_test_labels, hpc_predicted_labels, normalize=True,classes=class_names, title='Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "error_cnt = 0\n",
    "total_error_cnt = 0\n",
    "same_class_error = 0\n",
    "for i in range(len(clf_predicted_labels)):\n",
    "    if clf_predicted_labels[i] != clf_test_labels[i]:\n",
    "        total_error_cnt += 1\n",
    "        # if its normal data\n",
    "        if clf_predicted_labels[i] == 1 or clf_test_labels[i] == 1:\n",
    "            error_cnt += 1\n",
    "\n",
    "\n",
    "print (\"Total Error Count : \", total_error_cnt)\n",
    "print (\"Normal Class Error Rate : \", float(error_cnt)/float(len(clf_predicted_labels)) * 100, \"%\")\n",
    "print (\"Total Error Rate : \", float(total_error_cnt)/float(len(clf_predicted_labels)) * 100, \"%\")\n",
    "\n",
    "\n",
    "false_pos = 0\n",
    "false_neg = 0\n",
    "error_cnt = 0\n",
    "for i in range(len(clf_predicted_labels)):\n",
    "    # if there's an error\n",
    "    if clf_predicted_labels[i] != clf_test_labels[i]:\n",
    "        error_cnt += 1\n",
    "        # if we failed to detect anomaly\n",
    "        if clf_predicted_labels[i] == 1:\n",
    "            false_neg += 1\n",
    "        # detected anomaly, but it normal\n",
    "        else:\n",
    "            false_pos += 1\n",
    "print (\"Total Errors\", error_cnt)\n",
    "print (\"False Positives \", false_pos/ float(len(clf_predicted_labels)) * 100, \"%\")\n",
    "print (\"False Negatives \", false_neg/ float(len(clf_predicted_labels)) * 100, \"%\")\n",
    "\n",
    "\n",
    "clf_shortened_predicted = []\n",
    "clf_shortened_test_label = []\n",
    "for i in range(len(clf_predicted_labels)):\n",
    "    if clf_predicted_labels[i] == 1:\n",
    "        clf_shortened_predicted.append(1)\n",
    "    else:\n",
    "        clf_shortened_predicted.append(2)\n",
    "    if clf_test_labels[i] == 1:\n",
    "        clf_shortened_test_label.append(1)\n",
    "    else:\n",
    "        clf_shortened_test_label.append(2)\n",
    "\n",
    "plot_confusion_matrix(clf_shortened_test_label, clf_shortened_predicted, normalize=True,classes=short_class_names, \n",
    "                      title='Confusion Matrix', font_size=18, fig_size=(6,5))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
